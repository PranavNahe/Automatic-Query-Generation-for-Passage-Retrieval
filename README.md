Automatic query generation is a thriving field of research with numerous practical applications, especially in the domain of learning assessment. Learning outcomes serve as benchmarks for evaluating and comparing student performance in both online and traditional courses. Outcomes-based assessment plays a significant role in evaluating and improving academic degree programs, particularly in the context of ABET accreditation for engineering and computer science programs.

Taxonomies such as Bloom's cognitive domain and SOLO are employed to describe learning outcomes and assess performance on assessments. Test automation is an intriguing approach to overcome the time-consuming process of manually writing test cases. It offers benefits like reduced repetitive work, lower costs, and improved maintainability.

The correct configuration of the data tier is crucial to prevent data corruption, unexpected crashes, and security breaches. However, maintaining a perfect database in today's complex and extensive database systems poses significant challenges.

Obtaining "real" data from for-profit organizations is often the preferred choice, although real-world data has its limitations. The workload generated by actual queries may only explore a small portion of the design space, making it difficult to accurately predict the performance of additional query sets. Information retrieval concepts play a role in addressing these challenges.

A query is a specific request for data from a database, enabling users to identify trends, perform data edits, find records matching specific criteria, perform complex operations, and automate crucial data management procedures.

The expressive power of SQL, combined with optimizations and execution methods, presents an extensive landscape. As hardware advances, DBMS approaches incorporate new functionality to handle larger datasets and more complex queries. Consequently, evaluating a database engine's performance becomes a challenging task.

The quality and granularity of statistics play a crucial role in query generation and accuracy. Detailed statistics enable more precise and accurate query results.

In the context of this discussion, the focus is primarily on query generation for passage retrieval. Three algorithms utilizing query matching are employed to retrieve passages. Each experimental technique generates a query, which is then added to the query table. Matching the queries from the query table against the paragraphs in the input article allows for the annotation of the passage.

Considering examples, a subset of SQL grammar is examined, involving the selection of up to two table attributes from cross-joined tables in the student table. The grammar supports aggregate functions, with MAX and MIN as the only permitted options. The Alloy analyzer, based on SAT, converts Alloy formulas into Boolean formulas and enumerates potential solutions satisfying the model. A concretization program is utilized to convert the output from Alloy instances into complete SQL queries.

Furthermore, a real-time example of Grammarly is mentioned. Grammarly automatically detects typing mistakes, incorrect spellings, and highlights text where suitable paraphrasing content can be used.

Overall, automatic query generation has diverse applications and continues to evolve in various domains, facilitating efficient information retrieval and enhancing data management processes.

![image](https://github.com/PranavNahe/Automatic-Query-Generation-for-Passage-Retrieval/assets/81244950/5bd67f06-9e73-4f7e-b726-b0a064934834)



Proposed methodology

The system pipeline takes a text paragraph as input, processes it, and outputs the article with annotated passage. 
To accomplish this, various steps are followed using the NLP package called spaCy. First, the stopwords such as "a," "an," "the," "this," "that," "is," "it," "to," and "and" are removed from the input text. These stopwords generally have low information density and don't contribute significantly to the analysis.
Next, the text is processed using the pre-trained Natural Language Processing model called em_core_web_sm, which is one of the models provided by spaCy. 
The text is tokenized, meaning it is divided into individual tokens. During this process, punctuation marks, braces, parentheses, brackets, digits, floats, and other symbols are removed from the sentences.Once the tokens are created, the frequency of each word is calculated and stored in a frequency table. This information is used to determine the score of each token.The score is further normalized by dividing each token's frequency by the highest frequency among all the tokens. This normalized frequency represents the score of the tokens.Based on the token scores, the system generates Queries, which are the tokens with the highest scores. These Queries capture the most important and relevant information from the text.
With the Queries generated, the Passage generation process begins. The sentences in the text are tokenized as well, breaking them into individual tokens.The sentences that include the Queries are used to retrieve the passage. The sentence score is calculated based on the word scores, and the sentences with the highest scores are considered the most important. These sentences are chosen because they utilize the Queries generated earlier.
Finally, these important sentences are combined together in a meaningful manner, preserving the flow and coherence, and the resulting combination forms the retrieved passage.

Libraries.

spaCy library.

spaCy is an open-source library for natural language processing (NLP) in Python, known for its efficiency, scalability, and user-friendly design. It offers a wide range of key features and functionalities for processing text data. These include tokenization, part-of-speech (POS) tagging, named entity recognition (NER), dependency parsing, lemmatization, text classification, word vectors, and customization. With spaCy, developers and researchers can easily perform tasks like splitting text into tokens, assigning grammatical labels, identifying named entities, analyzing sentence structure, converting words to their base forms, classifying text, generating word vectors, and adapting models to specific domains or tasks. It is highly regarded for its speed and efficiency, thanks to its optimized implementation in Cython.

spaCy provides pre-trained models for multiple languages and domains, allowing users to immediately apply NLP techniques to their data. It also offers seamless integration with other Python libraries and frameworks, contributing to its versatility and making it a powerful tool for a wide range of NLP applications.

String library.

The `string` library in Python is a built-in module that offers various constants and functions for manipulating strings. It provides predefined constants like ASCII letters, digits, and case conversion functions. The module includes the `Template` class for creating string templates with placeholders and the `Formatter` class for advanced string formatting. Additionally, it offers functions for common string operations such as capitalizing words, stripping whitespace, and splitting strings. By importing and utilizing the `string` library, developers can simplify string manipulation tasks and perform operations efficiently.

Heapq library.

The `heapq` library in Python is a built-in module that enables the implementation of heap-based data structures, primarily used for tasks involving priority queues and efficient selection of smallest or largest elements. It provides functions for heap operations, maintaining the heap property where the smallest element is always at the top, as well as retrieving the N largest or N smallest elements from a collection. Additionally, it supports heap replacement, allowing for efficient replacement of the smallest element. The `heapq` module, part of the Python Standard Library, offers a straightforward and efficient solution for working with heap data structures and prioritizing elements based on their values, finding applications in tasks like task scheduling, top-k element selection, and priority queue implementation.	

Models

en_core_web_sm

`en_core_web_sm` is a small-sized pre-trained English language model available in the spaCy library, offering a wide range of NLP capabilities for processing English text. The model is specifically trained on English language data, and its "core" designation indicates that it covers essential NLP components like tokenization, part-of-speech tagging, named entity recognition, and syntactic dependency parsing. Being trained on web text, it is well-suited for processing text commonly found on the internet.

The "sm" in `en_core_web_sm` refers to its small size, making it a lightweight alternative compared to other models in spaCy. This small model is designed to be memory-efficient and offers faster processing speed, making it particularly useful in scenarios where resource limitations or quick response times are important considerations.To utilize the `en_core_web_sm` model in spaCy, you first need to install spaCy and then download the model using the command `python -m spacy download en_core_web_sm`. Once downloaded, you can load the model in your Python script using the command `spacy.load('en_core_web_sm')`.


Punctuations

In the string library in Python, the punctuation constant is a predefined string that contains all the ASCII punctuation characters. It includes the following characters: !"#$%&'()*+,-./:;<=>?@[\]^_{|}~`.
The punctuation constant is commonly used in string operations to identify and manipulate punctuation marks within a string. It can be helpful for tasks such as removing punctuation from a text, checking if a character is a punctuation mark, or splitting a string based on punctuation.

nlargest

The ‘nlargest’ function is a part of the ‘heapq’ module in Python's standard library. It is used to retrieve the n largest elements from a given iterable or a heap data structure.
The ‘nlargest’ function takes two arguments: ‘n’ and ‘iterable'. The ‘n’ parameter specifies the number of largest elements to be returned, while the ‘iterable' parameter represents the collection of elements from which the largest elements are to be selected.

